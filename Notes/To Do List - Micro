To Do List - Micro

0 - Fully test the system with the heartbeat
	- Big bug: it doesn't seem to worry about run time and will just always run all scans. Oops.
1 - Finish immediate scan function in scheduling
2 - Figure out how the parsing will work
	- Parsing: Regex to be performed on every line of the output file
	- returns a list of all parsed assets
	- These parsed assets get put into the database
	- Every time a new scan is completed, various alerting checks are made. This includes:
		
		- Anything that has never been seen before in a scan
		- Anything that has come up after being down for x amount of time / scans
		- Anything (content) that has changed - relevant with scans on single things.

	Which exposes an interesting point: we need to consider that there are two types of scans: single response and multi-response. As in, some scans will only have one asset (like on or off), whereas others will have multiple assets (like a list of content, or associated subdomains)

	The database stores assets in the following table:

	id:                   PRIMARY KEY
    target:               TEXT NOT NULL
    company:              TEXT NOT NULL
    asset_type:           TEXT NOT NULL
    asset_content:        TEXT NOT NULL
    scan_completed:       DATE NOT NULL
    scan_uuid             TEXT NOT NULL
    ignore                INT NOT NULL


Which means that we can get a list of all assets found in a single scan through the scan_id (or uuid, if we chose to implement that)


Issue:
We need to determine how the inputs and outputs works. We can get different inputs in the form of single targets or files - which can reference other scan types. 

Different inputs: 
- Discovered content
- Discovered domains
- Discovered IPs
- Single domain
- Single IP
Note that all of these must be in scope, so we'll need to check the scope on these.

Different outputs:
- Files on filesystem
- Results parsed into database for alerting
- Add to discovered content

One of the difficulties here is knowing what to put on the filesystem and what to have in the database.
Ideally, the database will have all the information, whilst the filesystem will have everything structured nicely to help with organisation.

Discovered Content is the important factor here. When it comes to looking at data, we need to find all unique things that have been discovered - content, domains, IPs, etc. The issue is that it's difficult to differentiate data in a way that is flexible and works for all edge cases. For example, we'll be performing the following kind of scans:

content_discovery:
 - Spidering
 - Parse JS files
 - gobuster

content_monitoring:
 - Check to see if the content_length of response has changed by more than 5%. 
 - INPUT: All discovered content
 - OUTPUT: content_length

visual_enumeration:
 - 


This is the list from General Musings:

Categories:
OSINT:
    - Whois
    - Is the IP Address a CDN?
    - Not really sure about this one
Subdomain enumeration
    - Amass
    - Subfinder
Domain Enumeration
    - Massdns + generated wordlist
Content Monitoring
    - Checking the content for changes in content-length (?)
    - Checking if port is still open (or, if previously closed, is open again)
    - Checking if an IP address is still alive (or, if previously non-live, is live again)
Content Discovery
    - Spidering (Need some kind of tool to do this)
    - Javascript Parsing
Content Bruteforcing
    - Gobuster / Recursebuster
    - Parameter bruteforcing (Parameth)
Visual Enumeration
    - Aquatone, screenshot EVERYTHING
    - Eyewitness(?)
Port Scan
    - nmap scripts on found open ports
    - massscan for open ports
Non-intrusive vuln scan
    - CDN Bypass
    - Subdomain Takeover
intrusive vuln scan
    - sqlmap
    - bxss
Credential bruteforce (probably not worth it):
    - brutespray

