To Do List - Micro

2 - Figure out how the parsing & assets will work
	- Parsing: Regex to be performed on every line of the output file
	- returns a list of all parsed assets
	- These parsed assets get put into the database
	- Every time a new scan is completed, various alerting checks are made. This includes:
		
		- Anything that has never been seen before in a scan
		- Anything that has come up after being down for x amount of time / scans
		- Anything (content) that has changed - relevant with scans on single things.

	Which exposes an interesting point: we need to consider that there are two types of scans: single response and multi-response. As in, some scans will only have one asset (like on or off), whereas others will have multiple assets (like a list of content, or associated subdomains)

	The database stores assets in the following table:

	id:                   PRIMARY KEY
    target:               TEXT NOT NULL
    company:              TEXT NOT NULL
    asset_type:           TEXT NOT NULL
    asset_content:        TEXT NOT NULL
    scan_completed:       DATE NOT NULL
    scan_uuid             TEXT NOT NULL
    ignore                INT NOT NULL


Which means that we can get a list of all assets found in a single scan through the scan_id (or uuid, if we chose to implement that)


================================================

Assets: data stuff

An asset is used for monitoring, for alerting and as input for other scans.

The most important and difficult factor is getting the output data to work with other tools.

Other tools have inputs such as the following:

full_urls.txt   (aquatone)
paths.txt       (gobuster)
single asset    (uptime check)

and the wordlists will sometimes have to be generated dynamically, too.

Yeesh.

So we've got the following different inputs:

INPUT
Single Asset
List of assets (Symbol deliniated)
Text file of assets - in different formats, as required.

Note that there is added complexity involving the actual formatting of the asset. 

WORDLIST
can be a wordlist from our filesystem or a wordlist text file of loads of assets
we'll have to generate the wordlist text file from assets from the database.


And we have different kinds of outputs:

True / False: in cases such as uptime checking
URL: containing part or all of scheme://host:port/path?query
Text: containing stuff like URLs and status codes.
Number: in cases like checking for content length 
Serialised Data: When we start performing vuln scans, we'll need information on the output of these. The serialised data can contain something like a base64 encoded HTTP request, or just a string. Not massively important for the time being, but worth being aware of.




And when we run a scan, there is an output file generated that will have the required information, but not formatted right.

So we'll have to regex out all of the assets, then stick them into the database with some kind of coherent formatting.

URLs are the most common type of asset, so there need to be some in-built parsing methods.

We need to really figure out all inputs to use. Like, are they just going to be IPs, domains and URLs? Or would I need to use some kind of content length as an input? 

Important Question: Is there any tool that has an input that isn't some form of IP, domain or URL?
No! 
This makes life easier. We just need to add asset_out_format and asset_in_format to the items accordingly and it should 



The table schema is as follows:


    id:                   PRIMARY KEY
    target:               TEXT NOT NULL
    company:              TEXT NOT NULL
    asset_type:           TEXT NOT NULL
        # Used to get the correct input for another scan
    asset_format:         TEXT NOT NULL
        # scheme://host:port/path?query
    asset_content:        TEXT NOT NULL
    scan_completed:       DATE NOT NULL
    scan_id               INTEGER NOT NULL
    ignore                INT NOT NULL



Target, company, scan completed and scan_id is pretty self explanatory.
asset_type: 
Different scans have different required input. For example, if an asset_type is 'discovered_subdomains' then it can be used by other tools that need subdomains as the input.
To have another tool use the output of a specific asset_type, the asset_type needs to match the input_type in the tools.json file of the tool you want to use the asset of. This needs to be written better to be made clearer.






Next Steps:


1 - Create tools.json properly: every tool in the below list needs to be integrated:

Subdomain enum: Amass
INPUT: scope
OUTPUT: discovered_hosts

Content Discovery: Recursebuster
INPUT: discovered_hosts
OUTPUT: dicovered_content

Content Changes: Gobuster
INPUT: discovered_content
OUTPUT: content_length

Visual Host Enumeration: Amass
INPUT: discovered_hosts
OUTPUT: web_document

Visual Content Enumeration: Amass
INPUT: discovered_content
OUTPUT: web_document

Domain IP monitor: nmap
INPUT: discovered_hosts
OUTPUT: discovered_ips

Port Scan: masscan
INPUT: discovered_ips
OUTPUT: discovered_ports

Port Check: nmap
INPUT: discovered_ports
OUTPUT: port_state (open or closed/filtered)
# Port check = check to see if previously discovered port is now open/closed

Service Enumeration: nmap
INPUT: discovered_ports
OUTPUT: service_details
# Using nmap's service detection scripts to see what is running on the ports.

CDN Bypass: custom script
INPUT: discovered_hosts
OUTPUT:


Note that in some cases, the tools will need wrapper scripts to perform some parsing.


Using the following template:
(filesystem is the new 'meta', even though we hadn't really integrated that yet.)

"domain_ip_monitor": {
        "command": "nmap -sP TARGET -oG OUTPUT",
        "intype": "single",
        "outtype": "single",
        "output": "discovered_ips",
        "input": "discovered_hosts",
        "informat": "host",
        "outformat": "host",
        "parse_result": "^Host: ([0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3})",
        "filesystem" : ""
        }

"content_length": {
        "command": "gobuster -u TARGET -w WORDLIST -o OUTPUT",
        "intype": "single",
        "outtype": "single",
        "output": "discovered_ips",
        "input": "discovered_hosts",
        "informat": "host",
        "outformat": "host",
        "parse_result": "^Host: ([0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3})",
        "filesystem" : ""
        "wordlist": {
            "file": "/usr/share/wordlists/SecLists/Notrelevantforthis",
            "input": "",
            "informat": "host"
            "intype": "file"

            }
        }

command:        the command used to perform the scan

input:          what asset to get from the database
informat:       how the tool wants the input URL
    i.e. scheme://host:port
intype:
    single      single asset
    file        create a temporary file of assets. TARGET = filename 
    list        comma delimited list of assets

output:         what to name the asset in the database
outformat:      what format the tool outputs the URL. 
    Note: this should be parsed into scheme://host:port/content
outtype:        what kind of output the tool gives
    single      single asset: only perform regex on group 1
    multi       multiple assets: need to loop through all regex groups
outfile:        the extention of the file created to parse (json, txt, gnmap etc. If blank, none added.)

parse_result:   the regular expression used to get the asset(s)

filesystem:     any custom filesystem interactions, using custom functions.

wordlist(dict)  wordlist to use. Can be dynamically generated. Parameter overridden by Scheduler if wordlist is supplied.
    file        if file is supplied in the object, use this as the wordlist.
    input       generate a wordlist from this asset (for the current target) from the database.
    intype      usually file, but can be single or list I guess(?)
    informat    same as base informat: what elements of the url to use and parse into a temp file.



Step-by-step for handling tools.json:

0. When a scope is added, add the scope items to the database as assets with the category/type 'scope'

1.  When a new schedule is added, check to see if it's a valid tool.
    How to check tool is valid:
        Check the following:
            Required params:
                command
                input
                informat
                intype
                output
                outformat
                outtype
                parse_results
            if WORDLIST in command, wordlist param is required.

2.  Put the raw tool info into the database in the scheduler.

3.  TO DO: Sort out the schedule database to allow for the dynamic wordlists to be generated and work with the new tools.josn

3.  The dynamic stuff happens at runtime. 
When a scan is about to be run by the scheduler, do the following:
    a. get schedule data from database
    b. look at input: get all data on input from database
    c. look at intype: different execution for 3 different types of intype.
        single: loop through assets and perform below functions on all of them
        list:   put all database assets into a list format and 
        file:   generate a file with all targets appropriately formatted and use this as the TARGET
    d. if wordlist, check if there is a 'file' in the tool or given through CLI
        otherwise, dynamically generate the wordlist file
    e. run scan
    f. parse results into database






2 - Should be a simple but nice thing to add - allow for schedules to have names!
3 - Create a function that handles tools.json appropriately.
    - This will involve:
    1: a function to check that it's a valid comodiac object (containing all the required params)
    2:  
3 - Tie the scripts together - make the functions that parse and change the in and out URL appropriately.