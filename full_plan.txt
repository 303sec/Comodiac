MVP for an enumeration platform:
Scheduled full subdomain discovery
Scheduled full host (IP address) discovery
Scheduled full content discovery - good wordlist.
Scheduled full port scanning
Create folders & oranize everything discovered
Alert system with Slack / Email if:
    new hosts are discovered
    new content is found
    new IP address found
    new open port discovered

How?:

Enter Scope -> ( Subdomain Discovery -> IPs parsed from Subdomains -> 
EyeWitness all subdomains & IP addresses -> IP enumeration (Reverse DNS / Lookup owner) -> 
Portscan all IPs -> Content Discovery -> Parameter Discovery ) = Added to Cron job! -> Vulnerability Scanning / Manual Analysis 

Enter Scope:
bugbot CLI
Available Commands:
-h = show this help
-v = verbose output
+   +   +   +   +   +   +   
-c =            Company Name
-t =            Targets in scope (10.10.10.10,www.paypal.com,*.this.com)
-T =            File containing targets in scope
-xt =           Targets not in scope
-xT =           File containing targets not in scope
-s =            Schedule (hourly/daily/weekly) - Each tool should have a default schedule.
--cronscan =    A flag to use when the scan is part of the crontab.  
-i =            immediately scan
--sync =        sync (using rsync) with a remote. Pushes any new data and then pulls any new data.
--cronsync =    Same as sync, but with the added option of adding to crontab.
--uninstall =   prompt user for: remove all cron jobs? create a tar archive of all scans? remove all files and folders? 
-l =            level of depth to the scanning:
Based on the tool_category: OSINT, subdomain_enumeration, etc.
1 - host discovery:
    - Get the IP addresses of all domains
    - Try to figure out IP address ownership
        (CDN? AWS? etc.)
    - Reverse DNS all non-CDN IP addresses found
    - Note: Do NOT scan CDNs. Try to find the origin IP.
    - Check for live non-CDN IP addresses
        - ICMP / top 100 ports TCP
    - wildcard subdomain enumeration
    - wildcard domain fragment enumeration
    - Make sure to put anything that isn't in scope in the out_of_scope folder




The useful thing about it being all wrapped up into a CLI means that it should be possible to create cron jobs for the command very easily. For example:

bugbot --cronscan -c test -t 127.0.0.1 -i -l 1

This would be the command to scan one of the hosts using level 1 (levels aren't quite decided on yet.)

I think that the -i flag is required for adding a scheduled... Or maybe it is removed and it's just implied that it's always there.

Only schedule a scan once the initial scan has worked and nothing broke.


How do levels/ does selecting the tool_category work?

Probably better to have a flag to pick category, and each category has an asociated number or letter.
As each category has to have a function to do the scans anyway,


Process =
1 - Start 

bugbot -c Paypal -t 10.10.10.10,www.paypal.com,*.this.com -xt

Creates the company directory, creates the scope folder. Creates 

Dir to start a test should have:
CompanyName/
    scope/
        inscope_domains.txt
        outofscope_domains.txt
        inscope_ips.txt
        outofscope_ips.txt

2 - Create discovered_hosts folder
CompanyName/
    discovered_hosts/
        domains.txt
        ips.txt
        out_of_scope/
            domains.txt
            ips.txt

3 - Add all scoped domains / ips to the domains /ip folder

4 - Parse any wildcard subdomain or domain fragments into separate files

4 - Perform init scans on the new / initial entries based on the level given.

wildcard subdomains
wildcard domain


x - Initial scans: 
    any wildcard subdomains? Sub-discovery. 
    Get IP addresses of given domains. 
    Any non-CDN IPs? Port scans on these. 
    CDN IPs? See if possible to bypass WAF/CDN 
    
    Content Discovery:
        - Spidering
        - JSParsing
        - Dir Brute - Gobuster / Recursebuster






Subdomain discovery:
When new subdomain is found:
    - create a SQLite DB for that subdomain.
    - create folder inside company folder
    - create placeholder folders for organization
    - trigger an alert
    - add subdomain to the base dir's subdomains.txt file



Root folder: Paypal
Directory: $base/paypal/www.paypal.com/
Database: www.paypal.com
Table: Uptime Monitor:

    Timestamp | Live | IP | 



Get information from the database and parse it into tool commands.
Get the output of tools and parse them into a database.

Individual wrappers need to be written for each tool that parses and gets information.

Folders created for each tool type.

Folder structure:

Paypal/
    paypal.db
    notes/
        <script to collect up notes folder on a daily basis, and add them to timestamped folders in this dir.>
    scope/
        inscope_domains.txt
        outofscope_domains.txt
        inscope_ips.txt
        outofscope_ips.txt
    discovery/
        discovered_domains.txt
        discovered_ips.txt
    targets/
        *.paypal.com/
            www.paypal.com/
                /

Example:

Requirement: An ideal command to use with placeholders:
e.g.
amass -active -brute -ip -src -d $domain -oA $dir/$domain


1 - Main logic:

How will scanning work?

Initial idea: a pre-made command as a string with input for output directory and other parameters.
Each command will have a function that can be called. A bit repetitive?

Second idea: a JSON file with the commands stored in a dictionary-like format:

{
    'amass_active': {
        'command':  'amass -active -brute -ip -src -d INPUT -oA OUTPUT',
        'input': 
        },
    'amass_passive': 'amass -passive -ip -src -T TIME -ef OUTOFSCOPE -df SCOPE -oA DIR'
    'parse_result': ''
} 

parse_result = the regular expression used to get each asset from the output file.

Company/
    /discovered
        /discovered_domains.txt
        /discovered_ips.txt
    /targets
        /folder
        /for
        /each
        /discovered
        /host

            /amass_passive
                010119/(date stamped folders)
            /amass_active
            /masscan
            /sublister
            /aquatone
            /eyewitness
            /etc.
            /latest (symlink to folder in /scans?)
                /the .txt of each scan
    /scope
        inscope_domains.txt
        outofscope_domains.txt
        inscope_ips.txt
        outofscope_ips.txt
        wildcard_domains.txt
        wildcard_fragments.txt


Initial scans:
-i = immediately scan
-


4 - Parse the file created and put the information into the database

[XML parser / JSON parser required]

open $dir/$domain
parse contents
add to database
remove file / store indefinitely on google drive

3 - 


'''


============================================================================================
DATABASE & ALERTING
============================================================================================
How the database and alerting will work:

Database has it's own parse_new_data function, not neccesarily required to be called when a tool is run.
Parsing the data comes in the form of reading the files and adding the results & file creation timestamp.
This allows for a database to be generated with just files, as a kind of init function if needed.

Each 'asset' is put into the table with a timestamp.
asset = open port / subdomain / live ip address / endpoint

For domains, we'll need to scan associated IP addresses & parse out any CDNs.

For IP addresses, we need to do reverse DNS lookups and get extra hostnames

e.g.
Domains

timestamp   {sql timestamp of scan}
tool        {tool}
IP          10.10.10.10
port        none
content     none
endpoint    none
screenshot  none

create a timestamp table containing:
tool | epoch_timestamp
parse_todays_scans executed
get all filenames in today's directory
get the timestamp from the filenames
compare the timestamps with the latest previous



Alerting checks:

New Port:
select $discovered_port from $target where ip = $ip
if it doesn't exist, alert on new port discovered.
[Note: must parse out any CDN ports first.]

New subdomain:
select $discovered_hostname from $target
if it doesn't exist, alert on new subdomain discovered.

New IP address from known hostname:
select * from $target where $discovered_ip = ip
[Note: must parse out any CDN ports first.]

New content discovered
select * from $target where $discovered_content = content

Content has changed
Options:
- Get 3-5 requests. Diff them, remove the difference. Hash the difference-less version
- Screenshot, see if screenshot is different by x%



===================================================================================================================
===================================================================================================================


# scrape subdomains
# run amass
# run sublister
# - iceman543/subfinder
# cloudflare enum


# dictionary-attacking subdomains
# massdns is real fast
# aquatone-scan is useful

# https://gist.github.com/jhaddix/
# - Contails a..txt for subdomains / content_discovery_all.txt for content

# Commonspeak & scans.io for some extra wordlists

# -------------

# Portscan = not with nmap, use massscan. Nmap = super slow
# masscan does not have host resolution:
#strip=$(echo $1 | sed 's/https\?:\/\///')
#masscan -p1-65535 $(dig +short $strip | grep -oE "\b([0-9]{1,3}\.{3}[0-9]{1,3}\b") | head -1) --max-rate 1000 |& tee $strip_scan

# brutespray for default passwords / anon logins remote admin protocols

# eyewitness --prepend-https : tries both http & https! Then screenshots.
# potential change in config to add more ports, which would be useful.

# Label sites with live/non-live and give an interesting rating out of 10.


#content discovery

#spider site - Go through all links

# parse javascript
# linkfinder
# jsparser

# directory bruting: use gobuster ... recursebuster
# Wordlists: robts.disallowed / robts.txt / raft / content_discovery_all.txt

# Parraameter bruting: parameth
# wordlist - backslash powered scanners top 2500 params

# exploits / testing
# blind xs - bxss
# idor - look for numerical parameters / hashes / email addresses

# subdomain takeover: can-itake-over-xyz

# cdn / waf bypass: find origin / find dev
# origin.sub.domain.com / origin-sub.domain.com / pragma:akamai-x-get-true-cache-key



# amass usage:

# amass -active -d $1 | tee <output>

# subfinder usage:

# subfinder -d $1 | tee <output>